{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf36af5",
   "metadata": {},
   "source": [
    "# Chatbot com RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d770a45",
   "metadata": {},
   "source": [
    "RAG (Retrieval Augmented Generation) é a estratégia aplicada a LLMs para gerar a resposta utilizando informações recuperadas de fontes externas a partir da consulta realizada. Neste caso, o prompt enviado ao LLM contém as informações recuperadas e a pergunta inicial.\n",
    "\n",
    "RAG é útil para combater o problema da alucinação em LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1b975",
   "metadata": {},
   "source": [
    "## Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f30d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c91d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2413825",
   "metadata": {},
   "source": [
    "## Componentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b99a59a",
   "metadata": {},
   "source": [
    "Define o LLM a ser usado para responder perguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\", \n",
    "    temperature=0.0, \n",
    "    max_tokens=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c444c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_groq import ChatGroq\n",
    "# llm = ChatGroq(\n",
    "#     model=\"llama3-8b-8192\", \n",
    "#     temperature=0.0, \n",
    "#     max_tokens=2000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc709c38",
   "metadata": {},
   "source": [
    "Define o modelo de embeddings usado para vetorizar os documentos.\n",
    "\n",
    "Veja mais em: https://www.sbert.net/docs/sentence_transformer/pretrained_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8fe0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"granite-embedding:278m\")\n",
    "vetorial_dbname = \"granite_embedding_278m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2490b79",
   "metadata": {},
   "source": [
    "Mais informações e outras opções de modelos de embeddings podem ser encontradas em:\n",
    "\n",
    "- https://python.langchain.com/docs/concepts/embedding_models/\n",
    "- https://python.langchain.com/docs/integrations/text_embedding/\n",
    "- https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "- https://arxiv.org/abs/2407.19527\n",
    "- https://huggingface.co/models?pipeline_tag=sentence-similarity&language=pt&sort=trending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620cca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# vetorial_dbname = \"nomic_embed_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac16f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"PORTULAN/serafim-100m-portuguese-pt-sentence-encoder-ir\")\n",
    "# vetorial_dbname = \"serafim_100m_ir\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769874a",
   "metadata": {},
   "source": [
    "Define o banco de dados vetorial que armazena os documentos e permite busca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"chatbot\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=f\"./chroma_db/{vetorial_dbname}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd40609",
   "metadata": {},
   "source": [
    "## [*] Indexa os documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50486546",
   "metadata": {},
   "source": [
    "Indexa os documentos na base vetorial, usando a representação de blocos de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d88fee",
   "metadata": {},
   "source": [
    "Lê os documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "loader = WikipediaLoader(query=\"Campinas\", load_max_docs=1, lang=\"pt\", doc_content_chars_max=-1)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"Documentos:\", len(docs))\n",
    "print(\"Tamanho (chars):\", len(docs[0].page_content))\n",
    "print(\"=\"*80, docs[0].page_content[:1000], \"[...]\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a7d74",
   "metadata": {},
   "source": [
    "Separa os documentos em blocos.\n",
    "\n",
    "Quando realizamos uma busca, o sistema recupera os blocos relacionados ao termo de busca.\n",
    "\n",
    "Isso permite respeitar o tamanho do contexto do LLM e também diminui o tempo de resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(\"Blocos:\", len(all_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed3c07",
   "metadata": {},
   "source": [
    "Adiciona os documentos na Vector Store.\n",
    "\n",
    "(descomente o código caso queira executar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b43f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "\n",
    "# print(\"vetorial_dbname:\", vetorial_dbname)\n",
    "\n",
    "# for doc in tqdm.tqdm(all_splits, desc=\"Adicionando documentos à Vector Store\"):\n",
    "#     vector_store.add_documents(documents=[doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761068e",
   "metadata": {},
   "source": [
    "## Recupera os documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82361401",
   "metadata": {},
   "source": [
    "Define o *retriever*, que recupera documentos (na verdade, blocos) a partir da representação da consulta realizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e4c4d",
   "metadata": {},
   "source": [
    "Recupera os documentos relacionados à consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4159f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def print_docs(docs: List[Document]):\n",
    "    print(\"### Documentos:\", len(docs))\n",
    "    print(\"=\"*80)\n",
    "    for doc in docs:\n",
    "        print(doc.page_content[:1000], \"=\"*80, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded758cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Usando o retriever:\", vetorial_dbname)\n",
    "\n",
    "# question = \"Qual a população de Campinas?\"\n",
    "# question = \"Quais os terminais de ônibus em Campinas?\"\n",
    "# question = \"Qual o papel da agricultura na economia de Campinas?\"\n",
    "# question = \"Quais são as universidades existentes em Campinas?\"\n",
    "# question = \"Quais os centros de pesquisa de Campinas?\"\n",
    "question = \"Quando a cidade foi fundada e qual era a base da economia?\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "print_docs(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aaa36f",
   "metadata": {},
   "source": [
    "## Aplica ReRanking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118b0da",
   "metadata": {},
   "source": [
    "Adicionando ReRanker.\n",
    "\n",
    "- https://python.langchain.com/docs/integrations/retrievers/flashrank-reranker/\n",
    "- https://python.langchain.com/docs/integrations/document_transformers/cross_encoder_reranker/ \n",
    "- https://github.com/PrithivirajDamodaran/FlashRank\n",
    "- https://www.sbert.net/examples/cross_encoder/applications/README.html\n",
    "- https://huggingface.co/Alibaba-NLP/gte-multilingual-reranker-base\n",
    "- https://huggingface.co/cross-encoder \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975064a",
   "metadata": {},
   "source": [
    "ReRanking rápido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb02398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors.flashrank_rerank import FlashrankRerank\n",
    "\n",
    "# Mantém somente os 3 melhores\n",
    "compressor = FlashrankRerank(model=\"ms-marco-MiniLM-L-12-v2\", top_n=3)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87022374",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = compression_retriever.invoke(question)\n",
    "print_docs(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd0df9",
   "metadata": {},
   "source": [
    "Outro tipo de modelo para ReRanking (mais lento):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers import ContextualCompressionRetriever\n",
    "# from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "# from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "# model = HuggingFaceCrossEncoder(\n",
    "#     model_name=\"Alibaba-NLP/gte-multilingual-reranker-base\",\n",
    "#     model_kwargs={\"trust_remote_code\": True},\n",
    "# )\n",
    "# compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "# base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# compression_retriever = ContextualCompressionRetriever(\n",
    "#     base_compressor=compressor, base_retriever=base_retriever\n",
    "# )\n",
    "\n",
    "# retrieved_docs = compression_retriever.invoke(question)\n",
    "# print_docs(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7d806a",
   "metadata": {},
   "source": [
    "## Realiza a consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4701d86d",
   "metadata": {},
   "source": [
    "Prepara o prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a2436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"Utilizando apenas as informações disponíveis, responda a pergunta.\n",
    "Se a pergunta não puder ser respondida a partir dessas informações, explique que não é possível responder a pergunta.\n",
    "\n",
    "### Informações disponíveis\n",
    "{context}\n",
    "\n",
    "### Pergunta\n",
    "{question}\n",
    "\n",
    "### Resposta\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_prompt(question: str, retrieved_docs: List[Document]):\n",
    "    # Monta o texto com todos os documentos recuperados\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    # Monta o prompt usando o template\n",
    "    prompt = prompt_template.format(question=question, context=docs_content)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8e0ac",
   "metadata": {},
   "source": [
    "Chama o LLM usando o prompt construído"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90caf480",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(question, retrieved_docs)\n",
    "response = llm.invoke(prompt)\n",
    "print(prompt)\n",
    "print(\"=\"*80)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764b0ef",
   "metadata": {},
   "source": [
    "Escreve a resposta token por token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5189e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d558bc",
   "metadata": {},
   "source": [
    "## Aplicação simples usando Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(question: str, history=None):\n",
    "    # O argumento history é passado pelo Gradio com o ChatInterface\n",
    "    retrieved_docs = compression_retriever.invoke(question)\n",
    "    prompt = build_prompt(question, retrieved_docs)\n",
    "    response = llm.invoke(prompt).content\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    return response, docs_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e538e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chatbot(\"Qual a população de Campinas?\")\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Chatbot com RAG\")\n",
    "    with gr.Accordion(\"Outras informações\", open=False):\n",
    "        retrieved_docs = gr.TextArea(label=\"Documentos Recuperados\")\n",
    "    gr.ChatInterface(\n",
    "        fn=chatbot,\n",
    "        additional_outputs=[retrieved_docs],\n",
    "        type=\"messages\",\n",
    "    )\n",
    "\n",
    "demo.launch(inline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d371810",
   "metadata": {},
   "source": [
    "Fecha a aplicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55fdaa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    demo.close()\n",
    "except NameError:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "using_langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
